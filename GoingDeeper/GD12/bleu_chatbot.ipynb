{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8c583a99",
   "metadata": {},
   "source": [
    "# Chatbot with BLUE score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "497e7fcc",
   "metadata": {},
   "source": [
    "__결과__\n",
    "  - 학습이 수렴하지 않아 대답을 생성하지 못한다.\n",
    "  - 이유는 아직 파악이 안 됨"
   ]
  },
  {
   "cell_type": "raw",
   "id": "6f81bfaf",
   "metadata": {},
   "source": [
    "# !pip install --upgrade gensim==3.8.3 ### 노션 참고\n",
    "\n",
    "Collecting gensim==3.8.3\n",
    "  Downloading gensim-3.8.3.tar.gz (23.4 MB)\n",
    "     |████████████████████████████████| 23.4 MB 4.5 MB/s            \n",
    "  Preparing metadata (setup.py) ... done\n",
    "Requirement already satisfied: numpy>=1.11.3 in /opt/conda/lib/python3.9/site-packages (from gensim==3.8.3) (1.21.4)\n",
    "Requirement already satisfied: scipy>=0.18.1 in /opt/conda/lib/python3.9/site-packages (from gensim==3.8.3) (1.7.1)\n",
    "Requirement already satisfied: six>=1.5.0 in /opt/conda/lib/python3.9/site-packages (from gensim==3.8.3) (1.16.0)\n",
    "Requirement already satisfied: smart_open>=1.8.1 in /opt/conda/lib/python3.9/site-packages (from gensim==3.8.3) (5.2.1)\n",
    "Building wheels for collected packages: gensim\n",
    "  Building wheel for gensim (setup.py) ... done\n",
    "  Created wheel for gensim: filename=gensim-3.8.3-cp39-cp39-linux_x86_64.whl size=24328218 sha256=c49ddb213cd83b9bb8014fb7218d261b11485a7c8ca967b22d591c9713669180\n",
    "  Stored in directory: /aiffel/.cache/pip/wheels/ca/5d/af/618594ec2f28608c1d6ee7d2b7e95a3e9b06551e3b80a491d6\n",
    "Successfully built gensim\n",
    "Installing collected packages: gensim\n",
    "  Attempting uninstall: gensim\n",
    "    Found existing installation: gensim 4.1.2\n",
    "    Uninstalling gensim-4.1.2:\n",
    "      Successfully uninstalled gensim-4.1.2\n",
    "Successfully installed gensim-3.8.3\n",
    "WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "d3798b88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.21.4\n",
      "1.3.3\n",
      "2.6.0\n",
      "3.6.5\n",
      "3.8.3\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import nltk\n",
    "import gensim\n",
    "# from gensim.models import KeyedVectors\n",
    "from konlpy.tag import Mecab\n",
    "from tqdm.notebook import tqdm\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "from nltk.translate.bleu_score import SmoothingFunction\n",
    "\n",
    "print(np.__version__)\n",
    "print(pd.__version__)\n",
    "print(tf.__version__)\n",
    "print(nltk.__version__)\n",
    "print(gensim.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "aa468d0a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ln: failed to create symbolic link '/aiffel/aiffel/gd_12/data/ChatbotData.csv': File exists\n",
      "ln: failed to create symbolic link '/aiffel/aiffel/gd_12/data/ko.bin': File exists\n",
      "ln: failed to create symbolic link '/aiffel/aiffel/gd_12/data/word2vec_ko.model': File exists\n",
      "ln: failed to create symbolic link '/aiffel/aiffel/gd_12/data/word2vec_ko.model.trainables.syn1neg.npy': File exists\n",
      "ln: failed to create symbolic link '/aiffel/aiffel/gd_12/data/word2vec_ko.model.wv.vectors.npy': File exists\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "256"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.system(\"mkdir -p ~/aiffel/gd_12/data/\")\n",
    "os.system(\"ln -s ~/data/* ~/aiffel/gd_12/data/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "8dddb3ad",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Q</th>\n",
       "      <th>A</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>12시 땡!</td>\n",
       "      <td>하루가 또 가네요.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1지망 학교 떨어졌어</td>\n",
       "      <td>위로해 드립니다.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3박4일 놀러가고 싶다</td>\n",
       "      <td>여행은 언제나 좋죠.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3박4일 정도 놀러가고 싶다</td>\n",
       "      <td>여행은 언제나 좋죠.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>PPL 심하네</td>\n",
       "      <td>눈살이 찌푸려지죠.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Q            A  label\n",
       "0           12시 땡!   하루가 또 가네요.      0\n",
       "1      1지망 학교 떨어졌어    위로해 드립니다.      0\n",
       "2     3박4일 놀러가고 싶다  여행은 언제나 좋죠.      0\n",
       "3  3박4일 정도 놀러가고 싶다  여행은 언제나 좋죠.      0\n",
       "4          PPL 심하네   눈살이 찌푸려지죠.      0"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d_path = \"/aiffel/aiffel/gd_12/data/\"\n",
    "raw_data = pd.read_csv(d_path+\"ChatbotData.csv\")\n",
    "raw_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "5f7a3806",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_sentence(sentence):\n",
    "    \n",
    "    # 모든 입력을 소문자로 변환; 알파벳, 문장부호, 한글만 남기고 모두 제거\n",
    "    sentence = re.sub(r\"[^0-9a-zA-Z가-힣ㄱ-ㅎㅏ-ㅣ\\?.!,]+\", \" \", sentence.lower())\n",
    "        \n",
    "    # 문장 앞뒤의 불필요한 공백을 제거합니다\n",
    "    sentence = sentence.strip()\n",
    "    sentence = re.sub(r'[\" \"]+', \" \", sentence)\n",
    "\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "37da7de7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "전체 샘플 수 : 11823\n",
      "전체 샘플 수 : 11823\n"
     ]
    }
   ],
   "source": [
    "questions = [str(s) for s in  raw_data[\"Q\"]]\n",
    "answers = [str(s) for s in  raw_data[\"A\"]]\n",
    "print('전체 샘플 수 :', len(questions))\n",
    "print('전체 샘플 수 :', len(answers))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c39b126",
   "metadata": {},
   "source": [
    "__데이터 전처리 및 토큰화__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "c6c32818",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Src, Tgt 한 쪽의 중복도 허용하지 않는 경우 예를들어,\n",
    "(\"A\", \"B\")\n",
    "(\"A\", \"D\")\n",
    "(\"C\", \"D\")\n",
    "위 경우 (\"A\", \"D\") 쌍이 먼저 포함되면\n",
    "(\"A\", \"B\")나 (\"C\", \"D\") 모두 사용할 수 없다.\n",
    "'''\n",
    "def build_corpus(src_raw, tgt_raw, max_len):\n",
    "    # 문장 전처리\n",
    "    src_corpus = [preprocess_sentence(s) for s in  src_raw]\n",
    "    tgt_corpus = [preprocess_sentence(s) for s in  tgt_raw]\n",
    "    \n",
    "    # 토큰화\n",
    "    src_corpus = [mecab_split(s) for s in  src_corpus]\n",
    "    tgt_corpus = [mecab_split(s) for s in  tgt_corpus]\n",
    "    \n",
    "    # 긴 문장 제외\n",
    "    filtered_idcs = [i for i, s in enumerate(zip(src_corpus, tgt_corpus))\n",
    "                             if len(s[0])<=max_len and len(s[1])<=max_len]\n",
    "    \n",
    "    # 중복문 제외\n",
    "    src_result, tgt_result = list(), list()\n",
    "    src_instnts, tgt_instnts = dict(), dict()\n",
    "    for i in filtered_idcs:\n",
    "        s = src_instnts.setdefault(src_corpus[i], []); s.append(i)\n",
    "        t = tgt_instnts.setdefault(tgt_corpus[i], []); t.append(i)\n",
    "        if len(t)>1: continue\n",
    "        if len(s)>1: continue\n",
    "        src_result.append(src_corpus[i])\n",
    "        tgt_result.append(tgt_corpus[i])\n",
    "    \n",
    "    return src_result, tgt_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "fc1443de",
   "metadata": {},
   "outputs": [],
   "source": [
    "mecab = Mecab()\n",
    "\n",
    "def mecab_split(sentence):\n",
    "    return tuple(mecab.morphs(sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "36ddd2a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "que_corpus , ans_corpus = build_corpus(questions, answers, max_len=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "1fd67712",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "전처리 샘플 수 : 7646\n",
      "전처리 샘플 수 : 7646\n"
     ]
    }
   ],
   "source": [
    "print('전처리 샘플 수 :', len(que_corpus))\n",
    "print('전처리 샘플 수 :', len(ans_corpus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "8456215a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "wv_path = \"/aiffel/aiffel/gd_12/\"\n",
    "model_dir = os.path.join(wv_path, 'ko.bin')\n",
    "\n",
    "# from gensim.models.keyedvectors import Word2VecKeyedVectors\n",
    "\n",
    "# wv = Word2VecKeyedVectors.load(model_dir)\n",
    "\n",
    "from gensim.models import Word2Vec\n",
    "ko_w2v = Word2Vec.load(model_dir)\n",
    "# model = Word2Vec.load_word2vec_format(model_dir, binary=True)\n",
    "\n",
    "# with open(model_dir, 'rb') as f:\n",
    "#     wv = Word2VecKeyedVectors.load_word2vec_format(\n",
    "#             f, binary=True, unicode_errors='ignore',) # , limit=500000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "888aabd7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('정직', 0.7132465839385986),\n",
       " ('겸손', 0.6951979398727417),\n",
       " ('완고', 0.6625800728797913),\n",
       " ('근면', 0.6611404418945312),\n",
       " ('친절', 0.6598716974258423),\n",
       " ('냉정', 0.6536936163902283),\n",
       " ('솔직', 0.6535051465034485),\n",
       " ('현명', 0.6468929648399353),\n",
       " ('충실', 0.6405693292617798),\n",
       " ('공평', 0.6393522620201111)]"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ko_w2v.wv.most_similar(\"성실\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdbc3633",
   "metadata": {},
   "source": [
    "__데이터 증강__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "d805a113",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lexical_sub(tokens, wv):\n",
    "    # tokens = sentence.split()\n",
    "\n",
    "    selected_tok = random.randrange(len(tokens))\n",
    "\n",
    "    result = []\n",
    "    for idx, tok in enumerate(tokens):\n",
    "        if idx is selected_tok:\n",
    "            try:\n",
    "                result.append(wv.most_similar(tok)[0][0])\n",
    "            # 로드된 word2vec 모델에 단어가 없을 경우\n",
    "            except KeyError as E:\n",
    "                return None\n",
    " \n",
    "        else:\n",
    "            result.append(tok)\n",
    "\n",
    "    return tuple(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "5f05887e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "398e0d040a4a42679257820d4643bea5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "new_src_tgt = ([], [])\n",
    "random.seed(25)\n",
    "\n",
    "for old_src, old_tgt in tqdm(zip(que_corpus, ans_corpus)):\n",
    "    new_src = lexical_sub(old_src, ko_w2v.wv)\n",
    "    new_tgt = lexical_sub(old_tgt, ko_w2v.wv)\n",
    "    if new_src is not None: \n",
    "        new_src_tgt[0].append(new_src)\n",
    "        new_src_tgt[1].append(old_tgt)\n",
    "    if new_tgt is not None: \n",
    "        new_src_tgt[0].append(old_src)\n",
    "        new_src_tgt[1].append(new_tgt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "5bc54d51",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13334 13334\n",
      "('12', '시', '땡', '캐치') ('하루', '가', '또', '가', '네요', '.')\n",
      "('12', '시', '땡', '!') ('일주일', '가', '또', '가', '네요', '.')\n",
      "('1', '중퇴', '학교', '떨어졌', '어') ('위로', '해', '드립니다', '.')\n",
      "('3', '박', '4', '일', '놀', '러', '가', '기에', '싶', '다') ('여행', '은', '언제나', '좋', '죠', '.')\n",
      "('3', '박', '4', '일', '놀', '러', '가', '고', '싶', '다') ('항해', '은', '언제나', '좋', '죠', '.')\n",
      "('ppl', '강하', '네') ('눈살', '이', '찌푸려', '지', '죠', '.')\n",
      "('sns', '들어맞', '팔', '왜', '안', '하', '지', 'ㅠㅠ') ('잘', '모르', '고', '있', '을', '수', '도', '있', '어요', '.')\n",
      "('sns', '맞', '팔', '왜', '안', '하', '지', 'ㅠㅠ') ('잘', '모르', '고', '있', '을', '수', '도', '있', '어요', '는데')\n",
      "('sns', '분간', '낭비', '인', '거', '아', '는데', '매일', '하', '는', '중') ('시간', '을', '정하', '고', '해', '보', '세요', '.')\n",
      "('sns', '시간', '낭비', '인', '거', '아', '는데', '매일', '하', '는', '중') ('시간', '을', '정하', '기에', '해', '보', '세요', '.')\n",
      "('sns', '보', '면', '나', '만', '빼', '고', '다', '사랑', '해', '보여') ('자랑', '하', '는', '자리', '니까요', '.')\n",
      "('sns', '보', '면', '나', '만', '빼', '고', '다', '행복', '해', '보여') ('자랑', '하', '는', '자리', 'ㄴ데요', '.')\n",
      "('가끔', '궁금하', '해') ('그', '사람', '도', '그럴', '거', '예요', '.')\n",
      "('가끔', '궁금', '해') ('그', '사람', '도', '그럴', '거', '예요', '는데')\n",
      "('가끔', '은데', '혼자', '인', '게', '좋', '다') ('혼자', '를', '즐기', '세요', '.')\n",
      "('가끔', '은', '혼자', '인', '게', '좋', '다') ('혼자', '를', '즐기', '세요', '는데')\n",
      "('가난', '한', '자마자', '의', '설움') ('돈', '은', '다시', '들어올', '거', '예요', '.')\n",
      "('가난', '한', '자', '의', '설움') ('돈', '은', '다시', '들어올', '것', '예요', '.')\n",
      "('가만', '있', '어도', '오줌', '난다') ('땀', '을', '식혀', '주', '세요', '.')\n",
      "('가만', '있', '어도', '땀', '난다') ('땀', '을', '식혀', '주', 'ㅂ시오', '.')\n"
     ]
    }
   ],
   "source": [
    "print(len(new_src_tgt[0]), len(new_src_tgt[1]))\n",
    "\n",
    "for i in range(20):\n",
    "    print(new_src_tgt[0][i], new_src_tgt[1][i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "bae23917",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "min_len:  1\n",
      "max_len:  32\n"
     ]
    }
   ],
   "source": [
    "enc_corpus = que_corpus + new_src_tgt[0]\n",
    "dec_corpus = ans_corpus + new_src_tgt[1]\n",
    "dec_corpus = [ [\"<bos>\"]+list(s)+[\"<eos>\"] for s in dec_corpus ]\n",
    "\n",
    "min_len = min(len(s) for s in enc_corpus+dec_corpus)\n",
    "max_len = max(len(s) for s in enc_corpus+dec_corpus)\n",
    "\n",
    "print(\"min_len: \", min_len)\n",
    "print(\"max_len: \", max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "80bb74ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "min_len = 3\n",
    "max_len = 30\n",
    "\n",
    "q_list, a_list = [], []\n",
    "\n",
    "for q, a in zip(enc_corpus, dec_corpus):\n",
    "    if len(q)<min_len: continue\n",
    "    if len(a)<min_len: continue   \n",
    "    if len(q)>max_len: continue\n",
    "    if len(a)>max_len: continue\n",
    "    q_list.append(q); a_list.append(a)\n",
    "\n",
    "enc_corpus, dec_corpus = q_list, a_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "d7640dd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20260 20260\n"
     ]
    }
   ],
   "source": [
    "print(len(enc_corpus), len(dec_corpus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "b85e3a8d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('12', '시', '땡', '!')\n"
     ]
    }
   ],
   "source": [
    "print(enc_corpus[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4873e7eb",
   "metadata": {},
   "source": [
    "__훈련-검증 분리__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "68b7bca2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "enc_train, enc_val, dec_train, dec_val = train_test_split(\n",
    "                                enc_corpus, dec_corpus, test_size=0.05, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "5012e701",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('나', '를', '좋아하', '는', '애', '랑', '친구', '가능', '?')\n"
     ]
    }
   ],
   "source": [
    "print(enc_train[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31b4e6c1",
   "metadata": {},
   "source": [
    "__토크나이저 사전 생성__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "bedaa1de",
   "metadata": {},
   "outputs": [],
   "source": [
    "ko_tokenizer = tf.keras.preprocessing.text.Tokenizer(\n",
    "                                filters='', lower=False, oov_token='')\n",
    "ko_tokenizer.fit_on_texts(list(s) for s in enc_train+dec_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "1747ae36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab Size: 6950\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['', '', '.', '<bos>', '<eos>', '이', '는', '하', '을', '가']"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Vocab Size:\", VOCAB_SIZE:=len(ko_tokenizer.index_word))\n",
    "# mecab_tokenizer.sequences_to_texts([mecab_tensor[100]])\n",
    "# tensor = tokenizer.texts_to_sequences(corpus)\n",
    "ko_tokenizer.sequences_to_texts([[i] for i in range(10)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "720cdbd6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1]]"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ko_tokenizer.texts_to_sequences([[\"<unk>\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "567ef096",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1]]"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ko_tokenizer.texts_to_sequences([[\"<uuuunk>\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "61a049bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1]]"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ko_tokenizer.texts_to_sequences([[\"\"]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f89332e",
   "metadata": {},
   "source": [
    "__시퀀스 패딩__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "37457875",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LEN = 30\n",
    "\n",
    "def padded_seqs(corpus, tokenizer):  # corpus: Tokenized Sentence's List\n",
    "    \n",
    "    if isinstance(corpus[0], list):\n",
    "        tensor = tokenizer.texts_to_sequences(corpus)\n",
    "    else: \n",
    "        tensor = tokenizer.texts_to_sequences(list(s) for s in corpus)\n",
    "    \n",
    "    tensor = tf.keras.preprocessing.sequence.pad_sequences(\n",
    "                                tensor, maxlen=MAX_LEN, padding='post')\n",
    "    return tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "bf035216",
   "metadata": {},
   "outputs": [],
   "source": [
    "enc_train = padded_seqs(enc_train, ko_tokenizer)\n",
    "enc_val = padded_seqs(enc_val, ko_tokenizer)\n",
    "dec_train = padded_seqs(dec_train, ko_tokenizer)\n",
    "dec_val = padded_seqs(dec_val, ko_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "49bca08f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  22   32  170    6   94  134   45  211   18    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0]\n",
      " [  96  560    2    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0]\n",
      " [1251    9    6   24  574   20   18    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0]]\n",
      "[[   3  820   90   70   36   33   49    2    4    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0]\n",
      " [   3 1343   41  369    9   14   64   22  171    2    4    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0]\n",
      " [   3 5922    8 5923  259   19   12    2    4    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0]]\n"
     ]
    }
   ],
   "source": [
    "print(enc_train[:3])\n",
    "print(dec_train[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "6636167d",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 128 # 64\n",
    "\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices(\n",
    "            (enc_train, dec_train) ).batch(batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "b0bc1dc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Positional Encoding 구현\n",
    "def positional_encoding(pos, d_model):\n",
    "    def cal_angle(position, i):\n",
    "        return position / np.power(10000, int(i) / d_model)\n",
    "\n",
    "    def get_posi_angle_vec(position):\n",
    "        return [cal_angle(position, i) for i in range(d_model)]\n",
    "\n",
    "    sinusoid_table = np.array([get_posi_angle_vec(pos_i) for pos_i in range(pos)])\n",
    "    sinusoid_table[:, 0::2] = np.sin(sinusoid_table[:, 0::2])\n",
    "    sinusoid_table[:, 1::2] = np.cos(sinusoid_table[:, 1::2])\n",
    "    \n",
    "    return tf.cast(sinusoid_table , tf.float32) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "6f5f4b83",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_padding_mask(seq):\n",
    "    seq = tf.cast(tf.math.equal(seq, 0), tf.float32)\n",
    "    return seq[:, tf.newaxis, tf.newaxis, :]\n",
    "\n",
    "def generate_causality_mask(size):\n",
    "    mask = 1 - tf.linalg.band_part(tf.ones((size, size)), -1, 0)\n",
    "    return mask  # (seq_len, seq_len)\n",
    "\n",
    "def generate_masks(src, tgt):\n",
    "    enc_mask = generate_padding_mask(src)\n",
    "    dec_enc_mask = generate_padding_mask(src)\n",
    "    \n",
    "    dec_padd_mask = generate_padding_mask(tgt)\n",
    "    causality_mask = generate_causality_mask(tgt.shape[1])\n",
    "    combined_mask = tf.maximum(dec_padd_mask, causality_mask) ###\n",
    "\n",
    "    return enc_mask, combined_mask, dec_enc_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "5a924cc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.d_model = d_model\n",
    "        \n",
    "        self.depth = d_model // self.num_heads\n",
    "        \n",
    "        self.W_q = tf.keras.layers.Dense(d_model) # Linear Layer\n",
    "        self.W_k = tf.keras.layers.Dense(d_model)\n",
    "        self.W_v = tf.keras.layers.Dense(d_model)\n",
    "        \n",
    "        self.linear = tf.keras.layers.Dense(d_model)\n",
    "\n",
    "    def scaled_dot_product_attention(self, Q, K, V, mask):\n",
    "        d_k = tf.cast(K.shape[-1], tf.float32)\n",
    "        \n",
    "        QK = tf.matmul(Q, K, transpose_b=True) # LMS 예시\n",
    "        scaled_qk = QK / tf.sqrt(d_k)\n",
    "        \n",
    "        if mask is not None: scaled_qk += (mask * -1e9)\n",
    "       \n",
    "        attentions = tf.nn.softmax(scaled_qk, axis=-1) # 예시 참고\n",
    "        out = tf.matmul(attentions, V)\n",
    "        \n",
    "        return out, attentions\n",
    "        \n",
    "    def split_heads(self, x):\n",
    "    \n",
    "        seq_len = x.shape[1]\n",
    "        split_x = tf.reshape(x,\n",
    "                         [-1, seq_len, self.num_heads, self.depth] )\n",
    "        split_x = tf.transpose(split_x, perm=[0, 2, 1, 3])\n",
    "        \n",
    "        return split_x\n",
    "\n",
    "    def combine_heads(self, x):\n",
    "  \n",
    "        seq_len = x.shape[2] # 예시 참고\n",
    "        combined_x = tf.transpose(x, perm=[0, 2, 1, 3])\n",
    "        combined_x = tf.reshape(combined_x,\n",
    "                         [-1, seq_len, self.d_model] )\n",
    "        \n",
    "        return combined_x\n",
    "    \n",
    "    def call(self, Q, K, V, mask):\n",
    "  \n",
    "        WQ = self.W_q(Q)\n",
    "        WK = self.W_q(K)\n",
    "        WV = self.W_q(V)\n",
    "        \n",
    "        WQ_split = self.split_heads(WQ) # 예시 참고\n",
    "        WK_split = self.split_heads(WK)\n",
    "        WV_split = self.split_heads(WV)\n",
    "        \n",
    "        out, attention_weights = self.scaled_dot_product_attention(\n",
    "                                    WQ_split, WK_split, WV_split, mask )\n",
    "        out = self.combine_heads(out)\n",
    "        out = self.linear(out)\n",
    "        \n",
    "        return out, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "fb27125a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PoswiseFeedForwardNet(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, d_ff):\n",
    "        super(PoswiseFeedForwardNet, self).__init__()\n",
    "        self.w_1 = tf.keras.layers.Dense(d_ff, activation='relu')\n",
    "        self.w_2 = tf.keras.layers.Dense(d_model)\n",
    "\n",
    "    def call(self, x):\n",
    "        out = self.w_1(x)\n",
    "        out = self.w_2(out)\n",
    "            \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "2c63dc1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, n_heads, d_ff, dropout):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "\n",
    "        self.enc_self_attn = MultiHeadAttention(d_model, n_heads)\n",
    "        self.ffn = PoswiseFeedForwardNet(d_model, d_ff)\n",
    "\n",
    "        self.norm_1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.norm_2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "\n",
    "        self.dropout = tf.keras.layers.Dropout(dropout)\n",
    "        \n",
    "    def call(self, x, mask, training):\n",
    "\n",
    "        residual = x\n",
    "        out = self.norm_1(x)\n",
    "        out, enc_attn = self.enc_self_attn(out, out, out, mask)\n",
    "        out = self.dropout(out, training=training)\n",
    "        out += residual\n",
    "        \n",
    "        residual = out\n",
    "        out = self.norm_2(out)\n",
    "        out = self.ffn(out)\n",
    "        out = self.dropout(out, training=training)\n",
    "        out += residual\n",
    "        \n",
    "        return out, enc_attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "b34f51df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DecoderLayer 클래스를 작성하세요.\n",
    "class DecoderLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, n_heads, d_ff, dropout):\n",
    "        super(DecoderLayer, self).__init__()\n",
    "\n",
    "        self.dec_self_attn = MultiHeadAttention(d_model, n_heads)\n",
    "        self.enc_dec_attn = MultiHeadAttention(d_model, n_heads)\n",
    "        self.ffn = PoswiseFeedForwardNet(d_model, d_ff)\n",
    "\n",
    "        self.norm_1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.norm_2_1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.norm_2_2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.norm_3 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "\n",
    "        self.dropout = tf.keras.layers.Dropout(dropout)\n",
    "        \n",
    "    def call(self, x1, x2, c_mask, mask, training):\n",
    "\n",
    "        residual = x1\n",
    "        norm_x1 = self.norm_1(x1)\n",
    "        out, dec_attn = self.dec_self_attn(norm_x1, norm_x1, norm_x1, c_mask) ###\n",
    "        out = self.dropout(out, training=training)\n",
    "        out += residual\n",
    " \n",
    "        residual = out\n",
    "        norm_x1 = self.norm_2_1(out)\n",
    "        norm_x2 = self.norm_2_2(x2) \n",
    "        out, dec_enc_attn = self.enc_dec_attn(norm_x1, norm_x2, norm_x2, mask)\n",
    "        out = self.dropout(out, training=training)\n",
    "        out += residual\n",
    "        \n",
    "        residual = out\n",
    "        out = self.norm_3(out)\n",
    "        out = self.ffn(out)\n",
    "        out = self.dropout(out, training=training)\n",
    "        out += residual\n",
    "        \n",
    "        return out, dec_attn, dec_enc_attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "fd5f3fbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(tf.keras.Model):\n",
    "    def __init__(self,\n",
    "                 n_layers,\n",
    "                 d_model,\n",
    "                 n_heads,\n",
    "                 d_ff,\n",
    "                 dropout):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.n_layers = n_layers\n",
    "        self.enc_layers = [EncoderLayer(d_model, n_heads, d_ff, dropout) \n",
    "                        for _ in range(n_layers)]\n",
    "        \n",
    "    def call(self, x, mask, training):\n",
    "        out = x\n",
    "    \n",
    "        enc_attns = list()\n",
    "        for i in range(self.n_layers):\n",
    "            out, enc_attn = self.enc_layers[i](out, mask, training)\n",
    "            enc_attns.append(enc_attn)\n",
    "        \n",
    "        return out, enc_attns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "a130b78e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(tf.keras.Model):\n",
    "    def __init__(self,\n",
    "                 n_layers,\n",
    "                 d_model,\n",
    "                 n_heads,\n",
    "                 d_ff,\n",
    "                 dropout):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.n_layers = n_layers\n",
    "        self.dec_layers = [DecoderLayer(d_model, n_heads, d_ff, dropout) \n",
    "                            for _ in range(n_layers)]\n",
    "                            \n",
    "                            \n",
    "    def call(self, x, enc_out, causality_mask, padding_mask, training):\n",
    "        out = x\n",
    "    \n",
    "        dec_attns = list()\n",
    "        dec_enc_attns = list()\n",
    "        for i in range(self.n_layers):\n",
    "            out, dec_attn, dec_enc_attn = \\\n",
    "              self.dec_layers[i](out, enc_out, causality_mask, padding_mask, training)\n",
    "            dec_attns.append(dec_attn)\n",
    "            dec_enc_attns.append(dec_enc_attn)\n",
    "\n",
    "        return out, dec_attns, dec_enc_attns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "1e95a65e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(tf.keras.Model):\n",
    "    def __init__(self,\n",
    "                 n_layers,\n",
    "                 d_model,\n",
    "                 n_heads,\n",
    "                 d_ff,\n",
    "                 src_vocab_size,\n",
    "                 tgt_vocab_size,\n",
    "                 pos_len,\n",
    "                 dropout=0.2,\n",
    "                 shared=True,\n",
    "                 shared_emb=False,):\n",
    "        super(Transformer, self).__init__()\n",
    "        \n",
    "        self.d_model = tf.cast(d_model, tf.float32)\n",
    "        self.shared = shared # 예시 참고\n",
    "        self.shared_emb = shared_emb # 예시 참고\n",
    "        \n",
    "        if shared_emb:\n",
    "            self.src_embedding = self.tgt_embedding = \\\n",
    "                        tf.keras.layers.Embedding(src_vocab_size, d_model)\n",
    "        else:\n",
    "            self.src_embedding = tf.keras.layers.Embedding(src_vocab_size, d_model)\n",
    "            self.tgt_embedding = tf.keras.layers.Embedding(tgt_vocab_size, d_model)\n",
    "        \n",
    "        self.pos_encoding = positional_encoding(pos_len, d_model) \n",
    "        \n",
    "        self.encoder = Encoder(n_layers, d_model, n_heads, d_ff, dropout)\n",
    "        self.decoder = Decoder(n_layers, d_model, n_heads, d_ff, dropout)\n",
    "        \n",
    "        self.linear = tf.keras.layers.Dense(tgt_vocab_size) # 예시 참고\n",
    "        \n",
    "        if shared : # 예시 참고\n",
    "            self.shared_weights = self.tgt_embedding.weights\n",
    "            self.linear.set_weights(tf.transpose(self.shared_weights))\n",
    "        \n",
    "        # self.dropout = tf.keras.layers.Dropout(dropout)\n",
    "\n",
    "    def embedding(self, emb, x, shared=False):\n",
    "  \n",
    "        seq_len = x.shape[1]\n",
    "        out = emb(x)\n",
    "        if shared: out *= tf.math.sqrt(self.d_model)\n",
    "        pos_enc = self.pos_encoding[np.newaxis, ...][:, :seq_len, :]\n",
    "        out = out + pos_enc\n",
    "        \n",
    "        return out\n",
    "\n",
    "    def call(self, enc_in, dec_in, enc_mask, combined_mask, dec_enc_mask, training):\n",
    "        \n",
    "        if self.shared_emb: enc_shared = self.shared\n",
    "        else: enc_shared = False\n",
    "        \n",
    "        enc_in = self.embedding(self.src_embedding, enc_in, enc_shared)\n",
    "        dec_in = self.embedding(self.tgt_embedding, dec_in, self.shared)\n",
    "        \n",
    "        enc_out, enc_attns = self.encoder(enc_in, enc_mask, training)\n",
    "        dec_out, dec_attns, dec_enc_attns = self.decoder(\n",
    "                            dec_in, enc_out, combined_mask, dec_enc_mask, training)\n",
    "        \n",
    "        logits = self.linear(dec_out)\n",
    "        \n",
    "        return logits, enc_attns, dec_attns, dec_enc_attns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "a3ec90a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "    from_logits=True, reduction='none')\n",
    "\n",
    "def loss_function(real, pred):\n",
    "    # 실제 시퀀스중 0인 값을 0, 나머지 1로 반환\n",
    "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "    loss_ = loss_object(real, pred)\n",
    "\n",
    "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "    # 실제 시퀀스중 0값을 갖는 위치의 손실에 0을 곱해줌\n",
    "    loss_ *= mask\n",
    "\n",
    "    # Masking 되지 않은 입력의 개수로 Scaling하는 과정\n",
    "    return tf.reduce_sum(loss_)/tf.reduce_sum(mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "d556eaf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LearningRateScheduler(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "    def __init__(self, d_model, warmup_steps=4000):\n",
    "        super(LearningRateScheduler, self).__init__()\n",
    "        self.d_model = d_model\n",
    "        self.warmup_steps = warmup_steps\n",
    "    \n",
    "    def __call__(self, step):\n",
    "        arg1 = step ** -0.5\n",
    "        arg2 = step * (self.warmup_steps ** -1.5)\n",
    "        \n",
    "        return (self.d_model ** -0.5) * tf.math.minimum(arg1, arg2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "id": "e8f38a63",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function()\n",
    "def train_step(src, tgt, model, optimizer):\n",
    "    gold = tgt[:, 1:]\n",
    "        \n",
    "    enc_mask, combined_mask, dec_enc_mask = generate_masks(src, tgt)\n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "        predictions, enc_attns, dec_attns, dec_enc_attns = \\\n",
    "                model(src, tgt, enc_mask, combined_mask, dec_enc_mask, training=True)\n",
    "        loss = loss_function(gold, predictions[:, :-1])\n",
    "\n",
    "\n",
    "    variables = model.trainable_variables\n",
    "    gradients = tape.gradient(loss, variables)\n",
    "    optimizer.apply_gradients(zip(gradients, variables))\n",
    "    \n",
    "    return loss, enc_attns, combined_mask, dec_enc_attns\n",
    "\n",
    "@tf.function  # 예제코드 재사용\n",
    "def eval_step(src, tgt, model):\n",
    "    gold = tgt[:, 1:]\n",
    "        \n",
    "    enc_mask, combined_mask, dec_enc_mask = generate_masks(src, tgt)\n",
    "    \n",
    "    predictions, enc_attns, dec_attns, dec_enc_attns = \\\n",
    "            model(src, tgt, enc_mask, combined_mask, dec_enc_mask, training=False)\n",
    "    \n",
    "    loss = loss_function(gold, predictions[:, :-1])\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "id": "ccc91cf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64 # 64\n",
    "\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices(\n",
    "            (enc_train, dec_train) ).batch(batch_size=BATCH_SIZE)\n",
    "\n",
    "N_LAYERS = 6\n",
    "D_MODEL = 256\n",
    "N_HEADS = 8\n",
    "D_FF = 2048\n",
    "pos_len = MAX_LEN # 40\n",
    "DROPOUT = 0.1\n",
    "\n",
    "transformer = Transformer(\n",
    "    n_layers=N_LAYERS,\n",
    "    d_model=D_MODEL,\n",
    "    n_heads=N_HEADS,\n",
    "    d_ff=D_FF,\n",
    "    src_vocab_size=VOCAB_SIZE,\n",
    "    tgt_vocab_size=VOCAB_SIZE,\n",
    "    pos_len=pos_len,\n",
    "    dropout=DROPOUT,\n",
    "    shared=True,\n",
    "    shared_emb=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "id": "6e704bbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = LearningRateScheduler(D_MODEL, warmup_steps=1000)\n",
    "optimizer = tf.keras.optimizers.Adam(#0.00001)\n",
    "                    learning_rate,beta_1=0.9,beta_2=0.98, epsilon=1e-9)\n",
    "# optimizer = tf.keras.optimizers.RMSprop(0.00001)\n",
    "#                         learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "id": "d66f3984",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "enc_mask: (64, 1, 1, 30)\n",
      "dec_enc_mask: (64, 1, 1, 30)\n",
      "dec_mask: (64, 1, 30, 30)\n",
      "enc_attns: (64, 8, 30, 30)\n",
      "Decoder Output: (64, 30, 6950)\n",
      "dec_attns: (64, 8, 30, 30)\n",
      "dec_enc_attns: (64, 8, 30, 30)\n",
      "Model: \"transformer_10\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_17 (Embedding)     multiple                  1779200   \n",
      "_________________________________________________________________\n",
      "encoder_10 (Encoder)         multiple                  7100928   \n",
      "_________________________________________________________________\n",
      "decoder_10 (Decoder)         multiple                  7896576   \n",
      "_________________________________________________________________\n",
      "dense_938 (Dense)            multiple                  1786150   \n",
      "=================================================================\n",
      "Total params: 18,562,854\n",
      "Trainable params: 18,562,854\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "sequence_len = 30\n",
    "\n",
    "sample_enc = tf.random.uniform((BATCH_SIZE, sequence_len))\n",
    "sample_dec = tf.random.uniform((BATCH_SIZE, sequence_len+7))\n",
    "\n",
    "sample_enc = tf.keras.preprocessing.sequence.pad_sequences(sample_enc, padding='post', maxlen=MAX_LEN)\n",
    "sample_dec = tf.keras.preprocessing.sequence.pad_sequences(sample_dec, padding='post', maxlen=MAX_LEN)\n",
    "\n",
    "enc_mask, combined_mask, dec_enc_mask = \\\n",
    "    generate_masks(sample_enc, sample_dec)\n",
    "\n",
    "sample_logits, enc_attns, dec_attns, dec_enc_attns = transformer(\n",
    "        sample_enc, sample_dec, enc_mask, combined_mask, dec_enc_mask )\n",
    "\n",
    "print ('enc_mask:', enc_mask.shape) # .shape\n",
    "print ('dec_enc_mask:', dec_enc_mask.shape) # .shape\n",
    "print ('dec_mask:', combined_mask.shape) # .shape\n",
    "print ('enc_attns:', enc_attns[0].shape) # .shape\n",
    "\n",
    "print ('Decoder Output:', sample_logits.shape)\n",
    "print ('dec_attns:', dec_attns[0].shape) # .shape\n",
    "print ('dec_enc_attns:', dec_enc_attns[0].shape) # .shape\n",
    "\n",
    "transformer.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "id": "f493c7f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_answer(tokens, model, src_tokenizer, tgt_tokenizer):\n",
    "    '''\n",
    "    translate와 동일\n",
    "    '''\n",
    "    padded_tokens = tf.keras.preprocessing.sequence.pad_sequences(\n",
    "                            [tokens], maxlen=MAX_LEN, padding='post' )\n",
    "    ids = []\n",
    "    # output = tf.expand_dims([tgt_tokenizer.bos_id()], 0)   \n",
    "    output = tf.expand_dims([tgt_tokenizer.word_index[\"<bos>\"]], 0)   \n",
    "    for i in range(MAX_LEN):\n",
    "        enc_padding_mask, combined_mask, dec_padding_mask = \\\n",
    "                                        generate_masks(padded_tokens, output)\n",
    "\n",
    "        predictions, _, _, _ = model(padded_tokens, \n",
    "                                      output,\n",
    "                                      enc_padding_mask,\n",
    "                                      combined_mask,\n",
    "                                      dec_padding_mask)\n",
    "\n",
    "        predicted_id = \\\n",
    "        tf.argmax(tf.math.softmax(predictions, axis=-1)[0, -1]).numpy().item()\n",
    "\n",
    "        # if tgt_tokenizer.eos_id() == predicted_id:\n",
    "        if tgt_tokenizer.word_index[\"<eos>\"] == predicted_id:\n",
    "            # result = tgt_tokenizer.decode_ids(ids)  \n",
    "            result = tgt_tokenizer.sequences_to_texts([ids])  \n",
    "            return result[0]\n",
    "\n",
    "        ids.append(predicted_id)\n",
    "        output = tf.concat([output, tf.expand_dims([predicted_id], 0)], axis=-1)\n",
    "\n",
    "    # result = tgt_tokenizer.decode_ids(ids)\n",
    "    result = tgt_tokenizer.sequences_to_texts([ids])  \n",
    "    \n",
    "    return result[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "id": "629d6f3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(dataset_count)\n",
    "run_idx = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "id": "c845650b",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert not np.any(np.isnan(enc_train))"
   ]
  },
  {
   "cell_type": "raw",
   "id": "d12ebe87",
   "metadata": {},
   "source": [
    "# 이전에 사용한 tf API 자료 입력\n",
    "dataset = tf.data.Dataset.from_tensor_slices((\n",
    "    {\n",
    "        'inputs': questions,\n",
    "        'dec_inputs': answers[:, :-1]\n",
    "    },\n",
    "    {\n",
    "        'outputs': answers[:, 1:]\n",
    "    },\n",
    "))\n",
    "\n",
    "EPOCHS = 30\n",
    "history = model.fit(dataset, epochs=EPOCHS, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "id": "8371790e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7f2633dc93ea4d599c6b2b70e481cdf7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/301 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a1236169d31c4939b1878bbba31f051b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/301 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e4ad9ca25f084328a14a1ddac408cccf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/301 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f43c8bbb79be427f916feaf051b74579",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/16 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "src:  지하철 만 타 면 생각나                         \n",
      "tgt:                               \n",
      "src:  너 누구 ...                           \n",
      "tgt:                               \n",
      "src:  6 개월 간 인의 짧 은 사랑 .                      \n",
      "tgt:                               \n",
      "src:  완전 재미 없 거든                          \n",
      "tgt:                               \n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 3\n",
    "for epoch in range(EPOCHS):\n",
    "    \n",
    "    total_loss = 0\n",
    "    \n",
    "    idx_list = list(range(0, len(enc_train), BATCH_SIZE)) # \n",
    "    random.shuffle(idx_list)\n",
    "    \n",
    "    t = tqdm(idx_list)\n",
    "    t.set_description_str('Epoch %2d' % (epoch + 1))\n",
    "\n",
    "    for (batch, idx) in enumerate(t):\n",
    "        if len(enc_train[idx:idx+BATCH_SIZE]) < BATCH_SIZE: continue\n",
    "        batch_loss, _, _, _ = train_step(enc_train[idx:idx+BATCH_SIZE], ##\n",
    "                                dec_train[idx:idx+BATCH_SIZE],\n",
    "                                transformer,\n",
    "                                optimizer)\n",
    "    \n",
    "        total_loss += batch_loss\n",
    "        # print(batch_loss.numpy())\n",
    "        # t.set_postfix_str('Loss %.4f' % float(batch_loss.numpy() ) )\n",
    "        t.set_postfix_str('Loss %.4f' % (total_loss.numpy() / (batch + 1)))\n",
    "\n",
    "    if (epoch+1) % 5 == 0: pass\n",
    "    elif epoch+1 == EPOCHS: pass\n",
    "    else: continue\n",
    "    \n",
    "    test_loss = 0\n",
    "    \n",
    "    idx_list = list(range(0, len(enc_val), BATCH_SIZE)) # \n",
    "    random.shuffle(idx_list)\n",
    "    t = tqdm(idx_list)\n",
    "    t.set_description_str('Test Epoch %2d' % (epoch + 1))\n",
    "    \n",
    "    for (test_batch, idx) in enumerate(t):\n",
    "        test_batch_loss = eval_step(enc_val[idx:idx+BATCH_SIZE],\n",
    "                                    dec_val[idx:idx+BATCH_SIZE],\n",
    "                                    transformer,)\n",
    "    \n",
    "        test_loss += test_batch_loss\n",
    "        t.set_postfix_str('Test Loss %.4f' % (test_loss.numpy() / (test_batch + 1)))\n",
    "    \n",
    "    for example in enc_val[:16:4]:\n",
    "        print(\"src: \", ko_tokenizer.sequences_to_texts([example])[0])\n",
    "        tgt = gen_answer(example, transformer, ko_tokenizer, ko_tokenizer)\n",
    "        print(\"tgt: \", tgt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab92cdea",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(enc_train[idx:idx+BATCH_SIZE]), batch, idx, BATCH_SIZE)\n",
    "\n",
    "for s in enc_train[idx:idx+BATCH_SIZE]:\n",
    "    print(s)\n",
    "    print(ko_tokenizer.sequences_to_texts([s])[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f8ec149",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_bleu_single(model, src_sentence, tgt_sentence, src_tokenizer, tgt_tokenizer, verbose=True):\n",
    "    \n",
    "    if isinstance(src_sentence[0], np.int32):\n",
    "        src_tokens = src_sentence\n",
    "        tgt_tokens = tgt_sentence\n",
    "        src_sentence = src_tokenizer.sequences_to_texts([src_tokens])[0]\n",
    "        tgt_sentence = tgt_tokenizer.sequences_to_texts([tgt_tokens])[0]\n",
    "    else:\n",
    "        src_tokens = src_tokenizer.texts_to_sequences([src_sentence])[0]\n",
    "        tgt_tokens = tgt_tokenizer.texts_to_sequences([tgt_sentence])[0]\n",
    "\n",
    "    if (len(src_tokens) > MAX_LEN): return None\n",
    "    if (len(tgt_tokens) > MAX_LEN): return None\n",
    "\n",
    "    reference = tgt_sentence.split()\n",
    "    candidate = gen_answer(src_tokens, model, src_tokenizer, tgt_tokenizer).split()\n",
    "\n",
    "    score = sentence_bleu([reference], candidate,\n",
    "                          smoothing_function=SmoothingFunction().method1)\n",
    "\n",
    "    if verbose:\n",
    "        print(\"Source Sentence: \", src_sentence)\n",
    "        print(\"Model Prediction: \", candidate)\n",
    "        print(\"Real: \", reference)\n",
    "        print(\"Score: %lf\\n\" % score)\n",
    "        \n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f3c487c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_bleu(model, src_sentences, tgt_sentence, src_tokenizer, tgt_tokenizer, verbose=True):\n",
    "    total_score = 0.0\n",
    "    sample_size = len(src_sentences)\n",
    "    \n",
    "    for idx in tqdm(range(sample_size)):\n",
    "        score = eval_bleu_single(model, src_sentences[idx], tgt_sentence[idx],\n",
    "                                 src_tokenizer, tgt_tokenizer, verbose)\n",
    "        if not score: continue\n",
    "        \n",
    "        total_score += score\n",
    "    \n",
    "    print(\"Num of Sample:\", sample_size)\n",
    "    print(\"Total Score:\", total_score / sample_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d18d248",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_bleu(reference, candidate, weights=[0.25, 0.25, 0.25, 0.25]):\n",
    "    return sentence_bleu([reference],\n",
    "                         candidate,\n",
    "                         weights=weights,\n",
    "                         smoothing_function=SmoothingFunction().method1)  # smoothing_function 적용\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "3dbea2e5",
   "metadata": {},
   "source": [
    "enc_val[test_idx][0], isinstance(enc_val[test_idx][0], np.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c32d9ea4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# test_idx = 0\n",
    "for test_idx in range(0,100,20):\n",
    "    eval_bleu_single(transformer, \n",
    "                     enc_val[test_idx], \n",
    "                     dec_val[test_idx], \n",
    "                     ko_tokenizer, \n",
    "                     ko_tokenizer)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "5411ed51",
   "metadata": {},
   "source": [
    "eval_bleu(transformer, enc_val, dec_val, ko_tokenizer, ko_tokenizer, verbose=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
